# setup 

```{r}
library(tidyverse)
library(readr)
library(dplyr)
library(randomForest)
library(tidymodels)
library(skimr)
library(pROC) # roc curves 
library(yardstick)
```

# loading data / preprocess data 
```{r}
tester <- read.csv("data/daymet/fieldweather_final.csv")
```

# training/testing set
```{r}
data <- tester %>%
    select(-lat, -lon, -irrigation) %>%
    mutate(
    # Convert the arrival delay to a factor
    factor_str_emmean = ifelse(str_emmean >= 30.45, "aboveMean", "belowMean"),
    factor_str_emmean = factor(factor_str_emmean)
    ) %>%
    na.omit() %>%
    mutate_if(is.character, as.factor)

set.seed(123) # change later?
data_split <- initial_split(data, prop = 3/4)
train_data <- training(data_split)
test_data <- testing(data_split)

#preprocsessing recipies
data_rec <-  # model_outcome ~ predictors 
    recipe(factor_str_emmean ~ .,data=train_data) %>%
    update_role(loc, year, new_role = "ID") %>%
    step_normalize(all_predictors())
    #keep loc/year but don't use as outcomes/predictors  
```

# logistics regression model 
```{r}
# build logistic regression model
lr_mod <- logistic_reg() %>% # can do linear instead linear_reg() %>% 
  set_engine("glm")

# bundeled model + recipie with workflows
data_wflow <- workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(data_rec)

data_wflow

# trained workflow
data_fit <- 
  data_wflow %>% 
  fit(data = train_data)

data_fit %>% 
#  extract_fit_parsnip() %>% 
   pull_workflow_fit() %>%
  tidy()

# use trained workflow to predict
predictions <- predict(data_fit, test_data)
predictions

data_aug <- 
  augment(data_fit, test_data)
data_aug

# roc curve # right now the model is not predicitn good (like randomly flipping a coin NOT good so work )
data_aug %>% 
  roc_curve(truth = factor_str_emmean, .pred_aboveMean) %>% 
  autoplot()
```


# using random forest model     CURRENT TEST
```{r}
data_Rf <- tester %>%
    select(-lat, -lon, -irrigation) %>%
    select(srad_mean_growingseason:prcp_sum_growingseason,  str_emmean)

data_Rf %>% 
  count(str_emmean) %>% 
  mutate(prop = n/sum(n))

# data splitting 
set.seed(123)
rf_split <- initial_split(data_Rf, 
                            strata = str_emmean)

rf_train <- training(rf_split)
rf_test  <- testing(rf_split)

# training
rf_mod <- 
  rand_forest(trees = 2000, mtry = sqrt(ncol(rf_train)) / 2, max_depth = 10) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

rf_train$str_emmean <- factor(rf_train$str_emmean)

rf_fit <- # takes a while to load since trees=1000
  rf_mod %>% 
  fit(str_emmean ~ ., data = rf_train )

rf_fit

```


# creating the model 
```{r}
df <- tester %>%
select(-loc, -lat, -lon, -irrigation, -year)

set.seed(123) # change later?
df_split <- initial_split(df, prop = 3/4)
train_df <- training(df_split)
test_df <- testing(df_split)

randomForest_model <- randomForest(str_emmean ~., data=train_df)

var_imp <- importance(randomForest_model)
var_imp
varImpPlot(randomForest_model)

#PLOT
# taller bars shows more important vars 
```


# filtering out those with high incnodepurity values 
```{r}
threshold <- 0.05  # Set your desired threshold value

# Filter out variables with high IncNodePurity values
important_vars <- names(var_imp[, "IncNodePurity"])[var_imp[, "IncNodePurity"] > threshold]

# Subset your data using the important variables
train_data_subset <- train_data[, names(train_data) %in% important_vars]


```

